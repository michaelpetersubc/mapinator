{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd84965-fcb1-4833-87df-aacc9b2f4bc2",
   "metadata": {},
   "source": [
    "## Mapinator Estimation Routines\n",
    "\n",
    "The five notebooks in this directory can be used to create a classification, then estimate the value of graduates from each tier.\n",
    "Each of them uses data saved to file from previous operations.  So you can create a classification without assembling the data (just read it from file)\n",
    "and you can estimate values without doing a classification.\n",
    "\n",
    "In logical order start with \n",
    "\n",
    "1. [mike_assemble_data](./mike_assemble_data.ipynb) - this cleans and assembles data using a routine written by James Yu and Silas Kwok\n",
    "1. [mike_create_classification](./mike_create_classification.ipynb) - creates a classification using code written by James Yu\n",
    "1. [mike_adjust_adjacency](./mike_adjust_adjacency.ipynb) - adjusts the adjacency matrix to account for bias in sampling (needs mysql and access to a server).\n",
    "1. [mike_hacked_estimator](./mike_hacked_estimator.ipynb) - estimates graduate values using the adjusted adjacency matrix  (based on code from James Yu\n",
    "1. [mike_fine_grained](./mike_fine_grained.ipynb) - an experimental effort to assign types to individual institutions based on the value of their hires (this is unfinished)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f41b0-b35b-4844-aa98-e005c451ee0e",
   "metadata": {},
   "source": [
    "## Contents of Estimates and Files\n",
    "\n",
    "The files in this folder consist of a combination of json files, julia encoded files that create fully formed julia datatypes when they are read, and an odd collection of `.tex` and `.png` files that can used when writing papers.\n",
    "\n",
    "### Data assembly\n",
    "\n",
    "The [mike_assemble_data](./mike_assemble_data.ipynb) creates a clean dataset by calling the api [support.econjobmarket.org](https://support.econjobmarket.org).  (The link doesn't load the data.  The appropriate endpoint is in the notebook file).\n",
    "\n",
    "This cleaned dataset is saved as a json file in the file [to_from_by_year.json](./current_estimates_and_files/to_from_by_year.json).  Subsequent workbooks process theh data by loading it from this file.\n",
    "\n",
    "### The Classification alorithm\n",
    "\n",
    "This notebook [mike_create_classification](./mike_create_classification.ipynb) run the classification algorithm. \n",
    "\n",
    "It begins by setting many of the parameters that are used in all the notebooks.  For example, the number of academic types is set here.  By appearance they are\n",
    "\n",
    "1. `show_tier_members` is set to 1 if you want the classification to list all the members of each tier at the end of the notebook output.  You might want to do this when you are experimenting, but generally there are better ways to do that descibed later. \n",
    "\n",
    "2. `NUMBER_OF_TYPES` sets the number of academic types to use in the classification.  Sinks are set separately.\n",
    "\n",
    "3. `sinks_to_include` manually sets the number of sinks to be included in the adjacency matrices.  Each element of the vector is the variable name of one of the sets that were declared in the previous lines.  You can comment out the set declarations in the previous lines if you don't include their name in this parameter vector.  But you don't have to do that.  If the sinks aren't named in the `sinks_to_include` parameter vector their data is just left out.  No sink name can be added to this vector unless a corresponding set has been initalized with that name.  Consider the sinks as created in the previous lines to be fixed.  Declaring a new set name then adding its name to the sinks to include parameter should trigger an exception.\n",
    "\n",
    "4. `algorithm_run_id` The only other parameter that can be set if the `algorithm_run_id`.  This number is set in the mysql database and is intended to give more details about the algorithm.  Code on the webserver can request information like the list of universities in the top tier, by specifying which run of the algorithm they want. \n",
    "\n",
    "Finally, a comment on the `unmatched` sink. The `unmatched` sink includes all the applicants who ended up getting jobs at places that didn't have an id number at econjobmarket, plus all the people who disappeard from the internet in the sense that no evidence of any job they had accepted could be found.  \n",
    "\n",
    "Roughly this list is probably the best guess about market failures.  An institution which is not listed on econjobmarket is one that has never advertised at econjobmarket, and which has never been listed by any applicant as their graduating institution.\n",
    "\n",
    "The best way to describe them is that they are institutions that do not participate in the international job market.  As such, graduates who take jobs with them are likely not to have received job offers on the international market.  These are currently the best guess of market failures.\n",
    "\n",
    "`Teaching Institutions` is a sink that is created automatically. It consists of all clearly academic institutions (Vassar is an example) who do not produce graduates (which means there is no record of a placement by any graduate from that institution).\n",
    "\n",
    "The algorithm reads the file `to_from_by_year.json` created by the data assembly worksheet.  It does not fetch new data from the api.  This is primarily to keep the data used by all the processing files consistent.\n",
    "\n",
    "#### Files Created by the Classification algorithm\n",
    "\n",
    "One of the other notebooks disaggregates the placements to compare individual institutions.  For this reason the algorithm creates two filtered copies of the data.  The algorithm saves a file called `filtered_data.jld`.  When this file is read back in to a julia program it comes our directly as a valid julia object.  In this case. loading the file will produce a julia Vector consisting of elements of type Any.  The actual elements are placements recorded in json.  They are all placements that have been filtered according to the settings described above.  Using this method the `filtered_data` file produces a julia type that is the same across all notebooks that use it.\n",
    "\n",
    "The program creates another file of type `.jld`  called `sinks.jld`.  This loads as a julia set consisting of a list of institution id numbers.  It is used in the `mike_likelihood_ratios.ipynb`.\n",
    "\n",
    "The next file saved is called `configuration_options.jld`.  It loads as a julia Dict.  This is an example:\n",
    "```\n",
    "Dict{String, Any} with 4 entries:\n",
    "  \"data_loaded\"        => DateTime(\"2024-06-18T18:21:53.103\")\n",
    "  \"institution_counts\" => [20, 58, 180, 334, 522, 152, 227, 598, 413, 1, 38, 64â€¦\n",
    "  \"algorithm_run_id\"   => 5\n",
    "  \"num_years\"          => 24\n",
    "```\n",
    "The `data_loaded` entry is created by looking at the date on the file itself.  The `num_years` entry is the number of years for which data are included in the `filtered_data` file.\n",
    "\n",
    "The program crates a `.jld` file called `placement_rates.jld` which loads as a julia Matrix of type Int32 and provides the adjacecny matrix associated with the best estimate of the classification.  The command `size(placement_rates)` produces a tuple.  The first element is the number of academic tiers plus the number of configured sinks.  The second element of the tuple is the number of academic tiers.  These parameters are set at the beginning of the create classification file. It is also how these size configuration numbers are passed on to other notebooks that use the data.\n",
    "\n",
    "A file called `id_to_type_api.json` is create.  Reading it in as a json file creates an array of dicts\n",
    "\n",
    "Finally, the program writes a latex file called `nice_adjacency_table.tex` which is suitable for reading in a lyx file or a latex file - in other words, when writing up stuff.\n",
    "\n",
    "### Summarizing Data\n",
    "\n",
    "The notebook [mike_likelihood_ratios](./mike_likelihood_ratios.ipynb) is an illustration of how the saved data from the classification can be referenced and used.  The only thing it does at the moment is to create a file called `likelihood_ratios.json` that creates a summary of properties gleaned from the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1900359b-0775-44e5-8ba3-2a45d7933591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
